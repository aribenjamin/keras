{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer Testbed\n",
    "\n",
    "#### This notebook tests optimization algorithms\n",
    "\n",
    "Tasks\n",
    "0. MNIST classification (CNN)\n",
    "0. CIFAR-10 classification (CNN)\n",
    "0. MNIST GAN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "#for plots\n",
    "def simpleaxis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.xaxis.set_tick_params(size=6)\n",
    "    ax.yaxis.set_tick_params(size=6)\n",
    "colors=['#F5A21E', '#02A68E', '#EF3E34', '#134B64', '#FF07CD']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification\n",
    "\n",
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train the model, plotting both train and test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First define a callback to record train and test loss on each minibatch\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class minibatch_History(Callback):\n",
    "    \"\"\"Callback that records events into a `History` object.\n",
    "    \n",
    "    Predicts over the validation set and each input batch (w/o dropout) \n",
    "    after each batch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, count_mode='samples', Nevery = 1):\n",
    "        super(minibatch_History, self).__init__()\n",
    "        self.Nevery = Nevery\n",
    "            \n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.batch = []\n",
    "        self.history = {'val_loss':list(),\n",
    "                        'val_acc':list()}\n",
    "        self.batch_no = 0\n",
    "        self.target = self.params['samples']\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.batch_no % self.Nevery == 0 or self.batch_no == self.target:\n",
    "            logs = logs or {}\n",
    "            self.batch.append(batch)\n",
    "            for k, v in logs.items():\n",
    "                self.history.setdefault(k, []).append(v)\n",
    "\n",
    "            # add validation loss. Only test on a random subset of minibatch size\n",
    "            val_loss, val_acc =  self.model.evaluate(self.validation_data[0], \n",
    "                                                     self.validation_data[1], verbose=0)   \n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.398905). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.319172). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.239439). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.246538). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.238179). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.236920). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.235179). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.233439). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.230760). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.229155). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.227607). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.234861). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.232126). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "keras/callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (1.228603). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "nb_train = 10000\n",
    "nb_test = 1000\n",
    "\n",
    "hist = minibatch_History(Nevery = 20)\n",
    "\n",
    "model.fit(x_train[:nb_train], y_train[:nb_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test[:nb_test], y_test[:nb_test]),\n",
    "          callbacks=[hist])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "simpleaxis(plt.gca())\n",
    "\n",
    "batch_no = hist.history['batch']\n",
    "plt.plot(batch_no,hist.history['loss'],color=colors[0])\n",
    "plt.plot(batch_no,hist.history['val_loss'],color=colors[1])\n",
    "plt.legend(['Train','Validation'],loc=0)\n",
    "plt.xlabel('Batch #',fontsize='large')\n",
    "plt.ylabel('Loss',fontsize='large')\n",
    "plt.ylim((0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the the training loss is higher because it is calculated from the net in training, which has dropout instantiated. TODO change the callback so this prints the actual loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot computational graphs\n",
    "\n",
    "Train a simple 2 layer linear net on a simple problem. Print the computation graph to error check.\n",
    "\n",
    "We'll be using TensorBoard, since it's awesome. This will print the log file for tensorboard; to see the graph and metrics you'll have to open the log in standalone tensorboard.\n",
    "\n",
    "Note that this requires us to be using tf as the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function: z = exp(-x^2 -y^2)\n",
    "\n",
    "x = np.random.normal(size = (1000,2))\n",
    "z = np.exp(-x[:,1]**2 -x[:,0]**2)\n",
    "\n",
    "x_test = np.random.normal(size = (100,2))\n",
    "z_test = np.exp(-x_test[:,1]**2 -x_test[:,0]**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_model = Sequential()\n",
    "\n",
    "small_model.add(Dense(2, activation='linear',input_shape=(2,)))\n",
    "small_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "opt = keras.optimizers.SGD()\n",
    "small_model.compile(loss=keras.losses.mean_squared_error,\n",
    "              optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for tensorboard\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "small_model.fit(x,z,\n",
    "          batch_size=10,\n",
    "          epochs=10,\n",
    "          verbose=0,\n",
    "          validation_data=(x_test, z_test),\n",
    "          callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x = np.array([[[1,2], [2,2]], [[3,2], [4,2]]])\n",
    "# y = np.array([[[5,2], [6,2]], [[7,2], [8,2]]])\n",
    "\n",
    "x = np.ones((30,2,3))\n",
    "y = x\n",
    "\n",
    "print(np.sum(np.multiply(x,y)))\n",
    "\n",
    "x = K.constant(x)\n",
    "y = K.constant(y)\n",
    "\n",
    "K.eval(K.sum(keras.backend.batch_dot(x,y,axes=(0,9)),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "np.sum(np.multiply(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot gradients\n",
    "#### As a function of inputs, and as a function of batch/epoch"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
